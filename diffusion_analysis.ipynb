{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from data_provider_pretrain.data_factory import data_provider\n",
    "from models.time_series_diffusion_model import TimeSeriesDiffusionModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from utils.callbacks import EMA\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import wandb\n",
    "from datetime import timedelta\n",
    "from utils.clean_args import clean_args\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Time-LLM')\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "torch.cuda.manual_seed(fix_seed)\n",
    "torch.cuda.manual_seed_all(fix_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"\n",
    "    A dictionary that supports both dot notation and dictionary access.\n",
    "    This allows both `args.att` and `args['att']` to work.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return self.get(attr)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self.__dict__[key] = value\n",
    "\n",
    "    def __delattr__(self, item):\n",
    "        self.__dict__.pop(item, None)\n",
    "\n",
    "default_config = DotDict({\n",
    "    # Basic config\n",
    "    \"num_nodes\": 1,\n",
    "    \"task_name\": \"long_term_forecast\",\n",
    "    \"is_training\": 1,\n",
    "    \"model_id\": \"ETTh1_ETTh2_512_192\",\n",
    "    \"model\": \"ns_Transformer\",\n",
    "    \"precision\": \"32\",\n",
    "    \n",
    "    # Data loader\n",
    "    \"data_pretrain\": \"Glucose\",\n",
    "    \"root_path\": \"/home/yl2428/Time-LLM/dataset/glucose\",\n",
    "    \"data_path\": \"combined_data_Jun_28.csv\",\n",
    "    \"data_path_pretrain\": \"combined_data_Jun_28.csv\",\n",
    "    \"features\": \"M\",\n",
    "    \"target\": \"OT\",\n",
    "    \"freq\": \"t\",\n",
    "    \"checkpoints\": \"/gpfs/gibbs/pi/gerstein/yl2428/checkpoints/\",\n",
    "    \"log_dir\": \"/gpfs/gibbs/pi/gerstein/yl2428/logs\",\n",
    "    \n",
    "    # Forecasting task\n",
    "    \"seq_len\": 128,\n",
    "    \"label_len\": 12,\n",
    "    \"pred_len\": 32,\n",
    "    \"seasonal_patterns\": \"Monthly\",\n",
    "    \"stride\": 8,\n",
    "    \n",
    "    # Model define\n",
    "    \"enc_in\": 4,\n",
    "    \"dec_in\": 4,\n",
    "    \"c_out\": 4,\n",
    "    \"d_model\": 32,\n",
    "    \"n_heads\": 8,\n",
    "    \"e_layers\": 2,\n",
    "    \"d_layers\": 1,\n",
    "    \"d_ff\": 128,\n",
    "    \"moving_avg\": 25,\n",
    "    \"factor\": 3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"embed\": \"timeF\",\n",
    "    \"activation\": \"gelu\",\n",
    "    \"output_attention\": False,\n",
    "    \"patch_len\": 16,\n",
    "    \"prompt_domain\": 0,\n",
    "    \"llm_model\": \"LLAMA\",\n",
    "    \"llm_dim\": 4096,\n",
    "    \n",
    "    # Optimization\n",
    "    \"num_workers\": 10,\n",
    "    \"itr\": 1,\n",
    "    \"train_epochs\": 100,\n",
    "    \"align_epochs\": 10,\n",
    "    \"ema_decay\": 0.97,\n",
    "    \"batch_size\": 64,\n",
    "    \"eval_batch_size\": 2,\n",
    "    \"patience\": 10,\n",
    "    \"learning_rate\": 0.0004,\n",
    "    \"des\": \"Exp\",\n",
    "    \"loss\": \"MSE\",\n",
    "    \"lradj\": \"COS\",\n",
    "    \"pct_start\": 0.2,\n",
    "    \"use_amp\": False,\n",
    "    \"llm_layers\": 32,\n",
    "    \"percent\": 100,\n",
    "    \"num_individuals\": -1,\n",
    "    \"enable_covariates\": 1,\n",
    "    \"cov_type\": \"tensor\",\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"use_deep_speed\": 1,\n",
    "    \n",
    "    # Wandb\n",
    "    \"wandb\": 1,\n",
    "    \"wandb_group\": None,\n",
    "    \"wandb_api_key\": \"6f1080f993d5d7ad6103e69ef57dd9291f1bf366\",\n",
    "    \"num_experts\": 8,\n",
    "    \"head_dropout\": 0.1,\n",
    "    \n",
    "    # TimeMixer-specific parameters\n",
    "    \"channel_independence\": 0,\n",
    "    \"decomp_method\": \"moving_avg\",\n",
    "    \"use_norm\": 1,\n",
    "    \"down_sampling_layers\": 2,\n",
    "    \"down_sampling_window\": 1,\n",
    "    \"down_sampling_method\": \"avg\",\n",
    "    \"use_future_temporal_feature\": 0,\n",
    "    \n",
    "    # Diffusion specific parameters\n",
    "    \"k_z\": 1e-2,\n",
    "    \"k_cond\": 1,\n",
    "    \"d_z\": 8,\n",
    "    \n",
    "    # De-stationary projector params\n",
    "    \"p_hidden_dims\": [64, 64],\n",
    "    \"p_hidden_layers\": 2,\n",
    "    \n",
    "    # CART related args\n",
    "    \"diffusion_config_dir\": \"/home/yl2428/Time-LLM/models/model9_NS_transformer/configs/toy_8gauss.yml\",\n",
    "    \"cond_pred_model_pertrain_dir\": None,\n",
    "    \"CART_input_x_embed_dim\": 32,\n",
    "    \"mse_timestep\": 0,\n",
    "    \"MLP_diffusion_net\": False,\n",
    "    \n",
    "    # Ax args\n",
    "    \"timesteps\": 1000,\n",
    "    \n",
    "    # Additional parameters\n",
    "    \"master_port\": 8889,\n",
    "    \"comment\": \"TimeLLM-ECL\"\n",
    "})\n",
    "\n",
    "\n",
    "args = default_config\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    train_data, train_loader, args = data_provider(args, args.data_pretrain, args.data_path_pretrain, True, 'train')\n",
    "    vali_data, vali_loader, args = data_provider(args, args.data_pretrain, args.data_path_pretrain, True, 'val')\n",
    "    test_data, test_loader, args = data_provider(args, args.data_pretrain, args.data_path_pretrain, True, 'test')\n",
    "    model = TimeSeriesDiffusionModel(args, train_loader, vali_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from data_provider_pretrain.data_factory import data_provider\n",
    "from models.time_series_flow_matching_model import TimeSeriesFlowMatchingModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from utils.callbacks import EMA\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import wandb\n",
    "from datetime import timedelta\n",
    "from utils.clean_args import clean_args\n",
    "import glob\n",
    "import re\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "torch.cuda.manual_seed(fix_seed)\n",
    "torch.cuda.manual_seed_all(fix_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"\n",
    "    A dictionary that supports both dot notation and dictionary access.\n",
    "    This allows both `args.att` and `args['att']` to work.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return self.get(attr)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self.__dict__[key] = value\n",
    "\n",
    "    def __delattr__(self, item):\n",
    "        self.__dict__.pop(item, None)\n",
    "\n",
    "# Flow matching configuration based on train_glucose_diffusion_slurm.sh\n",
    "flow_matching_config = DotDict({\n",
    "    # Basic config\n",
    "    \"num_nodes\": 1,\n",
    "    \"task_name\": \"long_term_forecast\",\n",
    "    \"is_training\": 1,\n",
    "    \"model_id\": \"ETTh1_ETTh2_512_192\",\n",
    "    \"model\": \"ns_Transformer\",  # From shell script\n",
    "    \"precision\": \"32\",\n",
    "    \"generative_model\": \"flow_matching\",  # Key difference from diffusion\n",
    "    \n",
    "    # Data loader (from shell script)\n",
    "    \"data_pretrain\": \"Glucose\",\n",
    "    \"root_path\": \"/home/yl2428/Time-LLM/dataset/glucose\",\n",
    "    \"data_path\": \"combined_data_Jun_28.csv\",\n",
    "    \"data_path_pretrain\": \"combined_data_Jun_28.csv\",\n",
    "    \"features\": \"MS\",  # From shell script\n",
    "    \"target\": \"OT\",\n",
    "    \"freq\": \"t\",\n",
    "    \"checkpoints\": \"/home/yl2428/checkpoints/\",\n",
    "    \"log_dir\": \"/home/yl2428/logs\",\n",
    "    \n",
    "    # Forecasting task (from shell script)\n",
    "    \"seq_len\": 72,\n",
    "    \"label_len\": 32,\n",
    "    \"pred_len\": 64,\n",
    "    \"seasonal_patterns\": \"Monthly\",\n",
    "    \"stride\": 1,  # From shell script\n",
    "    \n",
    "    # Model define (from shell script)\n",
    "    \"enc_in\": 4,\n",
    "    \"dec_in\": 4,\n",
    "    \"c_out\": 4,\n",
    "    \"d_model\": 32,  # From shell script\n",
    "    \"n_heads\": 8,\n",
    "    \"e_layers\": 2,\n",
    "    \"d_layers\": 1,\n",
    "    \"d_ff\": 256,  # From shell script\n",
    "    \"moving_avg\": 25,\n",
    "    \"factor\": 3,  # From shell script\n",
    "    \"dropout\": 0.1,\n",
    "    \"embed\": \"timeF\",\n",
    "    \"activation\": \"gelu\",\n",
    "    \"output_attention\": False,\n",
    "    \"patch_len\": 16,\n",
    "    \"prompt_domain\": 0,\n",
    "    \"llm_model\": \"LLAMA\",\n",
    "    \"llm_dim\": 4096,\n",
    "    \n",
    "    # VAE-specific parameters for ns_DLinear\n",
    "    \"latent_len\": 24,  # Half of seq_len by default\n",
    "    \"vae_hidden_dim\": 16,\n",
    "    \n",
    "    # Required for Trompt encoder - these will be populated by data_provider\n",
    "    \"col_stats\": None,\n",
    "    \"col_names_dict\": None,\n",
    "    \n",
    "    # Optimization (from shell script)\n",
    "    \"num_workers\": 10,\n",
    "    \"itr\": 1,\n",
    "    \"train_epochs\": 100,  # From shell script\n",
    "    \"align_epochs\": 10,\n",
    "    \"ema_decay\": 0.995,\n",
    "    \"batch_size\": 256,  # From shell script\n",
    "    \"eval_batch_size\": 256,\n",
    "    \"patience\": 10,\n",
    "    \"learning_rate\": 0.0001,  # From shell script\n",
    "    \"des\": \"Exp\",\n",
    "    \"loss\": \"MSE\",\n",
    "    \"lradj\": \"COS\",\n",
    "    \"pct_start\": 0.2,\n",
    "    \"use_amp\": False,\n",
    "    \"llm_layers\": 32,  # From shell script (llama_layers)\n",
    "    \"percent\": 100,\n",
    "    \"num_individuals\": 100,  # From shell script\n",
    "    \"enable_covariates\": 1,  # From shell script\n",
    "    \"cov_type\": \"tensor\",\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"use_deep_speed\": 1,  # From shell script\n",
    "    \n",
    "    # Wandb\n",
    "    \"wandb\": 1,\n",
    "    \"wandb_group\": None,\n",
    "    \"wandb_api_key\": \"6f1080f993d5d7ad6103e69ef57dd9291f1bf366\",\n",
    "    \n",
    "    # MoE parameters (from shell script)\n",
    "    \"use_moe\": 1,\n",
    "    \"num_experts\": 8,\n",
    "    \"top_k_experts\": 4,\n",
    "    \"moe_layer_indices\": [0, 1],\n",
    "    \"moe_loss_weight\": 0.01,\n",
    "    \"log_routing_stats\": 1,\n",
    "    \"num_universal_experts\": 1,\n",
    "    \"universal_expert_weight\": 0.3,\n",
    "    \"head_dropout\": 0.1,\n",
    "    \n",
    "    # TimeMixer-specific parameters\n",
    "    \"channel_independence\": 0,\n",
    "    \"decomp_method\": \"moving_avg\",\n",
    "    \"use_norm\": 1,\n",
    "    \"down_sampling_layers\": 2,\n",
    "    \"down_sampling_window\": 1,\n",
    "    \"down_sampling_method\": \"avg\",\n",
    "    \"use_future_temporal_feature\": 0,\n",
    "    \n",
    "    # Flow matching specific parameters\n",
    "    \"k_z\": 1e-2,\n",
    "    \"k_cond\": 1,\n",
    "    \"d_z\": 8,\n",
    "    \n",
    "    # De-stationary projector params\n",
    "    \"p_hidden_dims\": [64, 64],\n",
    "    \"p_hidden_layers\": 2,\n",
    "    \n",
    "    # Flow matching config\n",
    "    \"diffusion_config_dir\": \"/home/yl2428/Time-LLM/models/model9_NS_transformer/configs/toy_8gauss.yml\",\n",
    "    \"cond_pred_model_pertrain_dir\": None,\n",
    "    \"CART_input_x_embed_dim\": 32,\n",
    "    \"mse_timestep\": 0,\n",
    "    \"MLP_diffusion_net\": False,\n",
    "    \n",
    "    # Flow matching specific timesteps (reduced from 1000 for efficiency)\n",
    "    \"timesteps\": 50,\n",
    "    \n",
    "    # Flow matching ODE solver parameters\n",
    "    \"ode_solver\": \"dopri5\",\n",
    "    \"ode_rtol\": 1e-5,\n",
    "    \"ode_atol\": 1e-5,\n",
    "    \"interpolation_type\": \"linear\",\n",
    "    \"expert_layers\": 2,  # Add the missing expert_layers parameter\n",
    "})\n",
    "\n",
    "def find_best_checkpoint(base_path=\"/home/yl2428/logs/ns_DLinear/flow_matching\", metric=\"val_loss\"):\n",
    "    \"\"\"\n",
    "    Find the best checkpoint based on validation loss.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Base directory to search for checkpoints\n",
    "        metric: Metric to optimize (default: val_loss, lower is better)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (best_checkpoint_path, best_metric_value, run_name)\n",
    "    \"\"\"\n",
    "    print(f\"Searching for checkpoints in: {base_path}\")\n",
    "    \n",
    "    # Find all checkpoint directories\n",
    "    checkpoint_pattern = os.path.join(base_path, \"*/checkpoints/epoch=*-step=*-val_loss=*.ckpt/checkpoint\")\n",
    "    checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "    \n",
    "    if not checkpoint_dirs:\n",
    "        print(\"No checkpoints found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    best_checkpoint = None\n",
    "    best_metric = float('inf')  # Assuming lower is better for val_loss\n",
    "    best_run = None\n",
    "    \n",
    "    print(f\"Found {len(checkpoint_dirs)} checkpoints:\")\n",
    "    \n",
    "    for checkpoint_dir in checkpoint_dirs:\n",
    "        # Extract metric value from path\n",
    "        # Pattern: epoch=X-step=Y-val_loss=Z.ckpt\n",
    "        pattern = r'epoch=(\\d+)-step=(\\d+)-val_loss=([\\d.]+)\\.ckpt'\n",
    "        match = re.search(pattern, checkpoint_dir)\n",
    "        \n",
    "        if match:\n",
    "            epoch, step, val_loss = match.groups()\n",
    "            val_loss = float(val_loss)\n",
    "            \n",
    "            # Extract run name\n",
    "            run_name = checkpoint_dir.split('/')[-4]  # Get run directory name\n",
    "            \n",
    "            print(f\"  - {run_name}: epoch={epoch}, step={step}, val_loss={val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < best_metric:\n",
    "                best_metric = val_loss\n",
    "                best_checkpoint = checkpoint_dir\n",
    "                best_run = run_name\n",
    "    \n",
    "    if best_checkpoint:\n",
    "        print(f\"\\nBest checkpoint: {best_run}\")\n",
    "        print(f\"  - Path: {best_checkpoint}\")\n",
    "        print(f\"  - Val Loss: {best_metric:.4f}\")\n",
    "    \n",
    "    return best_checkpoint, best_metric, best_run\n",
    "\n",
    "def load_deepspeed_checkpoint(model, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load DeepSpeed checkpoint into the model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch Lightning model\n",
    "        checkpoint_path: Path to the DeepSpeed checkpoint directory\n",
    "    \n",
    "    Returns:\n",
    "        model: Model with loaded weights\n",
    "    \"\"\"\n",
    "    print(f\"Loading DeepSpeed checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    # DeepSpeed saves model states in mp_rank_00_model_states.pt\n",
    "    model_states_path = os.path.join(checkpoint_path, \"mp_rank_00_model_states.pt\")\n",
    "    \n",
    "    if not os.path.exists(model_states_path):\n",
    "        raise FileNotFoundError(f\"Model states file not found: {model_states_path}\")\n",
    "    \n",
    "    print(f\"Loading model states from: {model_states_path}\")\n",
    "    \n",
    "    # Determine the device to use\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(model_states_path, map_location=device)\n",
    "    \n",
    "    # Extract the model state dict\n",
    "    if 'module' in checkpoint:\n",
    "        state_dict = checkpoint['module']\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        # Sometimes the checkpoint is the state dict directly\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Remove any DeepSpeed prefixes if they exist\n",
    "    cleaned_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        # Remove common prefixes that DeepSpeed might add\n",
    "        clean_key = key\n",
    "        if key.startswith('_forward_module.'):\n",
    "            clean_key = key.replace('_forward_module.', '')\n",
    "        elif key.startswith('module.'):\n",
    "            clean_key = key.replace('module.', '')\n",
    "        \n",
    "        # Ensure the tensor is on the correct device\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            value = value.to(device)\n",
    "        \n",
    "        cleaned_state_dict[clean_key] = value\n",
    "    \n",
    "    # Load the state dict into the model\n",
    "    try:\n",
    "        # First move the model to the device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Load the state dict\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(cleaned_state_dict, strict=False)\n",
    "        \n",
    "        if missing_keys:\n",
    "            print(f\"Missing keys: {missing_keys[:10]}{'...' if len(missing_keys) > 10 else ''}\")\n",
    "        if unexpected_keys:\n",
    "            print(f\"Unexpected keys: {unexpected_keys[:10]}{'...' if len(unexpected_keys) > 10 else ''}\")\n",
    "            \n",
    "        print(\"‚úì Model weights loaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Some keys couldn't be loaded: {e}\")\n",
    "        # Try to load what we can\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in cleaned_state_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        \n",
    "        # Move model to device first\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(f\"‚úì Loaded {len(pretrained_dict)}/{len(cleaned_state_dict)} parameters\")\n",
    "    \n",
    "    # Ensure all submodules are on the correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Special handling for torch_frame components that might have device issues\n",
    "    def move_torch_frame_components_to_device(module, device):\n",
    "        \"\"\"Recursively move torch_frame components to device\"\"\"\n",
    "        for name, child in module.named_children():\n",
    "            if hasattr(child, 'fill_values') and isinstance(child.fill_values, torch.Tensor):\n",
    "                child.fill_values = child.fill_values.to(device)\n",
    "            if hasattr(child, 'embedding_table') and isinstance(child.embedding_table, torch.Tensor):\n",
    "                child.embedding_table = child.embedding_table.to(device)\n",
    "            # Recursively apply to children\n",
    "            move_torch_frame_components_to_device(child, device)\n",
    "    \n",
    "    # Apply device fix to the model\n",
    "    move_torch_frame_components_to_device(model, device)\n",
    "    \n",
    "    print(f\"‚úì All model components moved to {device}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def move_batch_to_device(batch, device):\n",
    "    \"\"\"\n",
    "    Move a batch of data to the specified device.\n",
    "    \n",
    "    Args:\n",
    "        batch: Batch data (can be tuple, list, tensor, or TensorFrame)\n",
    "        device: Target device\n",
    "    \n",
    "    Returns:\n",
    "        batch: Batch moved to device\n",
    "    \"\"\"\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        return type(batch)(move_batch_to_device(item, device) for item in batch)\n",
    "    elif isinstance(batch, torch.Tensor):\n",
    "        return batch.to(device)\n",
    "    elif hasattr(batch, 'to'):  # For TensorFrame and similar objects\n",
    "        return batch.to(device)\n",
    "    else:\n",
    "        return batch\n",
    "\n",
    "def load_flow_matching_model_with_weights(checkpoint_path=None, auto_find_best=True):\n",
    "    \"\"\"\n",
    "    Load and initialize the flow matching model with the specified configuration and weights.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Specific path to checkpoint directory (optional)\n",
    "        auto_find_best: If True, automatically find the best checkpoint (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, args, loaders, checkpoint_info)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data with flow matching config\n",
    "    flow_args = flow_matching_config\n",
    "    print(\"Loading data for Flow Matching model...\")\n",
    "    \n",
    "    train_data_fm, train_loader_fm, flow_args = data_provider(\n",
    "        flow_args, flow_args.data_pretrain, flow_args.data_path_pretrain, True, 'train'\n",
    "    )\n",
    "    vali_data_fm, vali_loader_fm, flow_args = data_provider(\n",
    "        flow_args, flow_args.data_pretrain, flow_args.data_path_pretrain, True, 'val'\n",
    "    )\n",
    "    test_data_fm, test_loader_fm, flow_args = data_provider(\n",
    "        flow_args, flow_args.data_pretrain, flow_args.data_path_pretrain, False, 'test'\n",
    "    )\n",
    "    \n",
    "    # Initialize Flow Matching model\n",
    "    print(\"Initializing Time Series Flow Matching Model...\")\n",
    "    flow_matching_model = TimeSeriesFlowMatchingModel(flow_args, train_loader_fm, vali_loader_fm, test_loader_fm)\n",
    "    \n",
    "    checkpoint_info = {}\n",
    "    \n",
    "    # Load weights if specified\n",
    "    if checkpoint_path or auto_find_best:\n",
    "        if auto_find_best and not checkpoint_path:\n",
    "            print(\"\\nFinding best checkpoint...\")\n",
    "            checkpoint_path, best_metric, run_name = find_best_checkpoint()\n",
    "            checkpoint_info = {\n",
    "                'path': checkpoint_path,\n",
    "                'val_loss': best_metric,\n",
    "                'run_name': run_name\n",
    "            }\n",
    "        \n",
    "        if checkpoint_path:\n",
    "            print(f\"\\nLoading weights from checkpoint...\")\n",
    "            flow_matching_model = load_deepspeed_checkpoint(flow_matching_model, checkpoint_path)\n",
    "            if not checkpoint_info:\n",
    "                checkpoint_info = {'path': checkpoint_path}\n",
    "        else:\n",
    "            print(\"No checkpoint found to load.\")\n",
    "    \n",
    "    print(\"‚úì Flow Matching model loaded successfully!\")\n",
    "    print(f\"  - Model type: {flow_args.model}\")\n",
    "    print(f\"  - Generative model: {flow_args.generative_model}\")\n",
    "    print(f\"  - ODE Solver: {flow_args.ode_solver}\")\n",
    "    print(f\"  - Timesteps: {flow_args.timesteps}\")\n",
    "    print(f\"  - Batch size: {flow_args.batch_size}\")\n",
    "    print(f\"  - Learning rate: {flow_args.learning_rate}\")\n",
    "    print(f\"  - MoE enabled: {flow_args.use_moe}\")\n",
    "    print(f\"  - Covariates enabled: {flow_args.enable_covariates}\")\n",
    "    print(f\"  - Model dimensions: d_model={flow_args.d_model}, d_ff={flow_args.d_ff}\")\n",
    "    print(f\"  - Sequence lengths: seq_len={flow_args.seq_len}, pred_len={flow_args.pred_len}\")\n",
    "    \n",
    "    if checkpoint_info:\n",
    "        print(f\"\\nCheckpoint info:\")\n",
    "        if 'run_name' in checkpoint_info:\n",
    "            print(f\"  - Run: {checkpoint_info['run_name']}\")\n",
    "        if 'val_loss' in checkpoint_info:\n",
    "            print(f\"  - Validation Loss: {checkpoint_info['val_loss']:.4f}\")\n",
    "        print(f\"  - Path: {checkpoint_info['path']}\")\n",
    "    \n",
    "    print(f\"\\nüìù Usage Tips:\")\n",
    "    print(f\"  - Use model.eval() before inference\")\n",
    "    print(f\"  - Move data to device: batch = move_batch_to_device(batch, model.device)\")\n",
    "    print(f\"  - For sampling: model.sample_step(batch, batch_idx)\")\n",
    "    \n",
    "    return flow_matching_model, flow_args, (train_loader_fm, vali_loader_fm, test_loader_fm), checkpoint_info\n",
    "\n",
    "def load_flow_matching_model():\n",
    "    \"\"\"Load and initialize the flow matching model with the specified configuration (without weights).\"\"\"\n",
    "    \n",
    "    # Load data with flow matching config\n",
    "    flow_args = flow_matching_config\n",
    "    print(\"Loading data for Flow Matching model...\")\n",
    "    \n",
    "    train_data_fm, train_loader_fm, flow_args = data_provider(\n",
    "        flow_args, flow_args.data_pretrain, flow_args.data_path_pretrain, True, 'train'\n",
    "    )\n",
    "    vali_data_fm, vali_loader_fm, flow_args = data_provider(\n",
    "        flow_args, flow_args.data_pretrain, flow_args.data_path_pretrain, True, 'val'\n",
    "    )\n",
    "    test_data_fm, test_loader_fm, flow_args = data_provider(\n",
    "        flow_args, flow_args.data_pretrain, flow_args.data_path_pretrain, True, 'test'\n",
    "    )\n",
    "    \n",
    "    # Initialize Flow Matching model\n",
    "    print(\"Initializing Time Series Flow Matching Model...\")\n",
    "    flow_matching_model = TimeSeriesFlowMatchingModel(flow_args, train_loader_fm, vali_loader_fm, test_loader_fm)\n",
    "    \n",
    "    print(\"‚úì Flow Matching model loaded successfully!\")\n",
    "    print(f\"  - Model type: {flow_args.model}\")\n",
    "    print(f\"  - Generative model: {flow_args.generative_model}\")\n",
    "    print(f\"  - ODE Solver: {flow_args.ode_solver}\")\n",
    "    print(f\"  - Timesteps: {flow_args.timesteps}\")\n",
    "    print(f\"  - Batch size: {flow_args.batch_size}\")\n",
    "    print(f\"  - Learning rate: {flow_args.learning_rate}\")\n",
    "    print(f\"  - MoE enabled: {flow_args.use_moe}\")\n",
    "    print(f\"  - Covariates enabled: {flow_args.enable_covariates}\")\n",
    "    print(f\"  - Model dimensions: d_model={flow_args.d_model}, d_ff={flow_args.d_ff}\")\n",
    "    print(f\"  - Sequence lengths: seq_len={flow_args.seq_len}, pred_len={flow_args.pred_len}\")\n",
    "    \n",
    "    return flow_matching_model, flow_args, (train_loader_fm, vali_loader_fm, test_loader_fm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, args, loaders, checkpoint_info = load_flow_matching_model_with_weights(checkpoint_path=\"/home/yl2428/logs/ns_Transformer/flow_matching/lilac-aardvark-389/checkpoints/epoch=9-step=23070-val_loss=1.8386.ckpt/checkpoint\")\n",
    "# model, args, loaders, checkpoint_info = load_flow_matching_model_with_weights(checkpoint_path=\"/home/yl2428/logs/ns_Transformer/flow_matching/lilac-aardvark-389/checkpoints/epoch=14-step=34605-val_loss=1.8891.ckpt/checkpoint\")\n",
    "model, args, loaders, checkpoint_info = load_flow_matching_model_with_weights(checkpoint_path=\"/home/yl2428/logs/ns_Transformer/flow_matching/rose-meadow-390/checkpoints/epoch=21-step=25058-val_loss=2.8174.ckpt/checkpoint\")\n",
    "train_loader, val_loader, test_loader = loaders\n",
    "\n",
    "print(\"\\nModel summary:\")\n",
    "print(f\"Flow matching model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Condition prediction model has {sum(p.numel() for p in model.cond_pred_model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='cuda',\n",
    "    devices=1, precision='64')\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"ns_Transformer\", name=\"test\")\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('/gpfs/gibbs/pi/gerstein/yl2428/logs/ns_Transformer/desert-sweep-6/checkpoints/checkpoints_1.pt')\n",
    "# turn into double\n",
    "for key in state_dict.keys():\n",
    "    state_dict[key] = state_dict[key].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample_outputs[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample_outputs[0]['batch_x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.sample_outputs, '/gpfs/gibbs/pi/gerstein/yl2428/logs/ns_Transformer/desert-sweep-6/checkpoints/sample_outputs_May11.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample_outputs = torch.load('/gpfs/gibbs/pi/gerstein/yl2428/logs/ns_Transformer/desert-sweep-6/checkpoints/sample_outputs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_frame import stype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = model.sample_outputs[0]['batch']\n",
    "batch_x, batch_y, batch_x_mark, batch_y_mark = batch[0]\n",
    "batch_cov = batch[1]\n",
    "# with torch.no_grad():\n",
    "#     new_batch_x = batch_x.clone()\n",
    "#     new_batch_x[53, :, 1] = batch_x[53, :, 1].min()\n",
    "#     model.eval()\n",
    "#     new_batch = [None, None]\n",
    "#     new_batch[0] = new_batch_x, batch_y, batch_x_mark, batch_y_mark\n",
    "#     new_batch[1] = batch_cov\n",
    "#     model.sample_step(new_batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_cov.feat_dict[stype.numerical][:,4 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.sample_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def plot_time_series_with_ci(groundtruth, sampled_output, cov, batch_x=None, num_series=5):\n",
    "    fig, axes = plt.subplots(num_series, 1, figsize=(12, 6*num_series), sharex=True)\n",
    "    if num_series == 1:\n",
    "        axes = [axes]\n",
    "    idx_list = [53, 11, 19]  # Adjust or randomize this list as needed\n",
    "    for i in range(num_series):\n",
    "        # Randomly select a time series from the batch\n",
    "        idx = idx_list[i]\n",
    "        hba1c = cov.feat_dict[stype.numerical][idx, 3]\n",
    "        diabetes_onset = cov.feat_dict[stype.numerical][idx, 1]\n",
    "        weight = cov.feat_dict[stype.numerical][idx, 4]\n",
    "        \n",
    "        if batch_x is not None:\n",
    "            # Extract previous glucose values\n",
    "            previous_glucose = batch_x[idx, :, -1].cpu().numpy()\n",
    "            hr = batch_x[idx, :, 0] * 20.41707644 + 7.93461185e+01\n",
    "            steps = batch_x[idx, :, 1] * 20.84327263 +  6.53019535e+00\n",
    "            print(steps)\n",
    "            hr_mean = np.mean(hr.cpu().numpy())\n",
    "            steps_mean = np.sum(steps.cpu().numpy())\n",
    "            \n",
    "            # Concatenate previous glucose with groundtruth and mean predictions\n",
    "            full_groundtruth = np.concatenate([previous_glucose, groundtruth[idx, :, -1]])\n",
    "        \n",
    "        else:\n",
    "            full_groundtruth = groundtruth[idx, :, -1].cpu().numpy()\n",
    "\n",
    "        # Plot ground truth (concatenated)\n",
    "        axes[i].plot(full_groundtruth, color='#1f77b4', label='Ground Truth (with previous)', lw=2)\n",
    "        \n",
    "        # Add textual information\n",
    "        axes[i].text(0, 2.8, f'idx: {idx}, hba1c: {hba1c.cpu().numpy():.2f}, diabetes_onset: {diabetes_onset.cpu().numpy():.2f}, weight: {weight.cpu().numpy():.2f}, steps: {steps_mean:.2f}', \n",
    "                     fontsize=12, color='black', bbox=dict(facecolor='white', alpha=0.5))\n",
    "        \n",
    "        # Calculate mean and confidence interval for predicted values\n",
    "        mean = np.mean(sampled_output[idx, :, :, 0], axis=0)\n",
    "        ci_lower = np.percentile(sampled_output[idx, :, :, 0], 80, axis=0)\n",
    "        ci_upper = np.percentile(sampled_output[idx, :, :, 0], 20, axis=0)\n",
    "        \n",
    "        # Smooth the CI with a moving average\n",
    "        ci_lower_smooth = uniform_filter1d(ci_lower, size=5)\n",
    "        ci_upper_smooth = uniform_filter1d(ci_upper, size=5)\n",
    "        \n",
    "        # Concatenate previous glucose with predicted mean and CI\n",
    "        full_mean = np.concatenate([previous_glucose, mean])\n",
    "        full_ci_lower = np.concatenate([previous_glucose, ci_lower_smooth])\n",
    "        full_ci_upper = np.concatenate([previous_glucose, ci_upper_smooth])\n",
    "        \n",
    "        # Plot mean prediction (concatenated)\n",
    "        axes[i].plot(full_mean, color='#ff7f0e', label='Mean Prediction (with previous)', lw=2)\n",
    "        \n",
    "        # Plot confidence interval (concatenated)\n",
    "        axes[i].fill_between(range(full_mean.shape[0]), full_ci_lower, full_ci_upper, color='#ff7f0e', alpha=0.3, label='95% CI')\n",
    "        \n",
    "        axes[i].set_title(f'Time Series {i+1}', fontsize=14)\n",
    "        axes[i].set_xlabel('Time Step', fontsize=12)\n",
    "        axes[i].set_ylabel('Value', fontsize=12)\n",
    "        \n",
    "        # Set y limit to be the same for all plots\n",
    "        axes[i].set_ylim([-3, 3])\n",
    "        axes[i].legend(loc='upper right', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.savefig('time_series_with_ci.pdf')\n",
    "    plt.show()\n",
    "\n",
    "# Sample invocation of the function with your data\n",
    "# plot_time_series_with_ci(groundtruth, sampled_output, cov, num_series=5)\n",
    "\n",
    "j = 0\n",
    "groundtruth_to_plot = model.sample_outputs[j]['true']\n",
    "sampled_output_to_plot = model.sample_outputs[j]['pred']\n",
    "cov_to_plot = model.sample_outputs[j]['batch_cov']\n",
    "batch_x_to_plot = model.sample_outputs[j]['batch_x']\n",
    "# Call the function to plot 2 random time series\n",
    "plot_time_series_with_ci(groundtruth_to_plot, sampled_output_to_plot, cov_to_plot, batch_x_to_plot, num_series=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the specific slice you intend to modify\n",
    "original_tensor_slice = batch_cov_orig.feat_dict[stype.numerical][:, 3]\n",
    "modified_slice = original_tensor_slice.clone() * 1.1\n",
    "batch_cov_orig.feat_dict[stype.numerical][:, 3] = modified_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the specific slice you intend to modify\n",
    "original_tensor_slice = batch_cov_orig.feat_dict[stype.numerical][:, 3]\n",
    "modified_slice = original_tensor_slice.clone() * 1.1\n",
    "batch_cov_orig.feat_dict[stype.numerical][:, 3] = modified_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the specific slice you intend to modify\n",
    "original_tensor_slice = batch_cov_orig.feat_dict[stype.numerical][:, 3]\n",
    "modified_slice = original_tensor_slice.clone() * 1.1\n",
    "batch_cov_orig.feat_dict[stype.numerical][:, 3] = modified_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the specific slice you intend to modify\n",
    "original_tensor_slice = batch_cov_orig.feat_dict[stype.numerical][:, 3]\n",
    "modified_slice = original_tensor_slice.clone() * 1.1\n",
    "batch_cov_orig.feat_dict[stype.numerical][:, 3] = modified_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_frame import stype # Ensure this is consistent with how stype is used/imported earlier\n",
    "\n",
    "# Ensure 'model' is loaded and model.sample_outputs is populated from previous cells.\n",
    "# For example, if needed:\n",
    "# model.sample_outputs = torch.load('/gpfs/gibbs/pi/gerstein/yl2428/logs/ns_Transformer/desert-sweep-6/checkpoints/sample_outputs.pt')\n",
    "\n",
    "def perturb_hba1c_covariates(batch_cov, individual_indices, percentage_increase):\n",
    "    \"\"\"\n",
    "    Perturbs HBA1c for specified individuals in the batch_cov.\n",
    "    HBA1c is assumed to be at index 3 of the numerical features based on notebook analysis.\n",
    "    Input batch_cov is expected to be a torch_frame.MaterializedFrame object or similar.\n",
    "    Returns a new batch_cov object with perturbations; does not modify the input object.\n",
    "    \"\"\"\n",
    "    if stype.numerical not in batch_cov.feat_dict:\n",
    "        print(f\"Warning: stype.numerical ('{stype.numerical}') not found in batch_cov.feat_dict. Returning original batch_cov.\")\n",
    "        return batch_cov\n",
    "\n",
    "    # Clone the numerical features tensor to ensure modifications do not affect the original batch_cov\n",
    "    original_numerical_tensor = batch_cov.feat_dict[stype.numerical]\n",
    "    perturbed_numerical_tensor = original_numerical_tensor.clone()\n",
    "    \n",
    "    for idx in individual_indices:\n",
    "        if 0 <= idx < perturbed_numerical_tensor.shape[0]:\n",
    "            # Modify the cloned tensor\n",
    "            current_hba1c_val = perturbed_numerical_tensor[idx, 3]\n",
    "            perturbed_numerical_tensor[idx, 3] = current_hba1c_val * (1 + percentage_increase / 100.0)\n",
    "            # print(f\"Individual {idx}: HbA1c changed from {current_hba1c_val.item():.2f} to {perturbed_numerical_tensor[idx, 3].item():.2f}\")\n",
    "        else:\n",
    "            print(f\"Warning: Index {idx} is out of bounds for numerical_feats (shape: {perturbed_numerical_tensor.shape}). Skipping perturbation for this index.\")\n",
    "            \n",
    "    from copy import deepcopy\n",
    "    new_feat_dict = deepcopy(batch_cov) \n",
    "    # \n",
    "    new_feat_dict.feat_dict[stype.numerical] = perturbed_numerical_tensor\n",
    "    \n",
    "    # Prepare arguments for constructing the new frame object.\n",
    "    # It's important to pass all necessary attributes from the original batch_cov\n",
    "    # that are required by its constructor (e.g., col_names_dict, col_stats).\n",
    "    \n",
    "    return new_feat_dict\n",
    "\n",
    "\n",
    "def perturb_weight_covariates(batch_cov, individual_indices, percentage_increase):\n",
    "    \"\"\"\n",
    "    Perturbs HBA1c for specified individuals in the batch_cov.\n",
    "    HBA1c is assumed to be at index 3 of the numerical features based on notebook analysis.\n",
    "    Input batch_cov is expected to be a torch_frame.MaterializedFrame object or similar.\n",
    "    Returns a new batch_cov object with perturbations; does not modify the input object.\n",
    "    \"\"\"\n",
    "    if stype.numerical not in batch_cov.feat_dict:\n",
    "        print(f\"Warning: stype.numerical ('{stype.numerical}') not found in batch_cov.feat_dict. Returning original batch_cov.\")\n",
    "        return batch_cov\n",
    "\n",
    "    # Clone the numerical features tensor to ensure modifications do not affect the original batch_cov\n",
    "    original_numerical_tensor = batch_cov.feat_dict[stype.numerical]\n",
    "    perturbed_numerical_tensor = original_numerical_tensor.clone()\n",
    "    \n",
    "    for idx in individual_indices:\n",
    "        if 0 <= idx < perturbed_numerical_tensor.shape[0]:\n",
    "            # Modify the cloned tensor\n",
    "            current_hba1c_val = perturbed_numerical_tensor[idx, 4]\n",
    "            perturbed_numerical_tensor[idx, 4] = current_hba1c_val * (1 + percentage_increase / 100.0)\n",
    "            # print(f\"Individual {idx}: HbA1c changed from {current_hba1c_val.item():.2f} to {perturbed_numerical_tensor[idx, 3].item():.2f}\")\n",
    "        else:\n",
    "            print(f\"Warning: Index {idx} is out of bounds for numerical_feats (shape: {perturbed_numerical_tensor.shape}). Skipping perturbation for this index.\")\n",
    "            \n",
    "    from copy import deepcopy\n",
    "    new_feat_dict = deepcopy(batch_cov) \n",
    "    # \n",
    "    new_feat_dict.feat_dict[stype.numerical] = perturbed_numerical_tensor\n",
    "    \n",
    "    # Prepare arguments for constructing the new frame object.\n",
    "    # It's important to pass all necessary attributes from the original batch_cov\n",
    "    # that are required by its constructor (e.g., col_names_dict, col_stats).\n",
    "    \n",
    "    return new_feat_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Perturbation Analysis ---\n",
    "# Select a batch for analysis (e.g., the last one processed or a specific one)\n",
    "# If model.sample_outputs is a list of outputs from trainer.test:\n",
    "# Each element in model.sample_outputs would typically be a dictionary\n",
    "# from a single batch processed by test_step.\n",
    "# We'll use the last batch's data as an example.\n",
    "# You might need to adjust which batch or how data is selected based on your exact structure.\n",
    "model.cuda()\n",
    "if not model.sample_outputs:\n",
    "    print(\"Error: model.sample_outputs is empty. Please ensure the model has processed data and populated this list.\")\n",
    "else:\n",
    "    # Assuming the structure seen in the notebook: model.sample_outputs[i]['batch']\n",
    "    # and model.sample_outputs[i]['pred'] for predictions.\n",
    "    # We need an original batch to get 'batch_x', 'batch_y', 'batch_x_mark', 'batch_y_mark', and 'batch_cov'.\n",
    "    \n",
    "    # Let's use the data from the last entry in sample_outputs for perturbation\n",
    "    # This corresponds to the data used for the last plot in the notebook (j = -1)\n",
    "    # Or you can select a specific batch index, e.g., batch_index_to_perturb = 0\n",
    "    batch_index_to_perturb = 0 # Use the last batch by default\n",
    "    \n",
    "    original_batch_data_dict = model.sample_outputs[batch_index_to_perturb]\n",
    "    original_batch_tuple = original_batch_data_dict['batch'] # This is [ (batch_x, batch_y, batch_x_mark, batch_y_mark), batch_cov ]\n",
    "    \n",
    "    batch_x_orig, batch_y_orig, batch_x_mark_orig, batch_y_mark_orig = original_batch_tuple[0]\n",
    "    batch_cov_orig = original_batch_tuple[1]\n",
    "\n",
    "    # Get original predictions (samples before perturbation)\n",
    "    # These are the 'pred' values from the model's output for this original batch\n",
    "    # Assuming 'pred' stores the 50 samples: [batch_size, num_samples, pred_len, num_features]\n",
    "    # And we are interested in the glucose feature, which is the last one (index -1 or 3 for c_out=4)\n",
    "    sampled_output_before_perturb = original_batch_data_dict['pred'][..., -1] # Taking only glucose\n",
    "\n",
    "    num_individuals_in_batch = batch_x_orig.shape[0]\n",
    "    num_to_perturb = min(256, num_individuals_in_batch) # Perturb up to 3 individuals, or fewer if batch is small\n",
    "\n",
    "    # Randomly select individuals to perturb\n",
    "    # Ensure reproducibility if desired, by setting random.seed elsewhere or here for this specific selection\n",
    "    # random.seed(42) # for reproducibility of selection\n",
    "    individuals_to_perturb_indices = random.sample(range(num_individuals_in_batch), num_to_perturb)\n",
    "    print(f\"Original batch size: {num_individuals_in_batch}\")\n",
    "    print(f\"Randomly selected individuals to perturb (indices): {individuals_to_perturb_indices}\")\n",
    "\n",
    "    # Perturb HBA1c for the selected individuals\n",
    "    percentage_increase_hba1c = 20.0\n",
    "    batch_cov_perturbed = perturb_weight_covariates(batch_cov_orig, individuals_to_perturb_indices, percentage_increase_hba1c)\n",
    "\n",
    "    # Prepare the new batch for the model's sample_step or equivalent generation function\n",
    "    # The model.sample_step(batch, batch_idx) was used in the notebook\n",
    "    # We need to simulate how samples are generated or find the appropriate generation function.\n",
    "    # If model.sample_step appends to model.sample_outputs, we need to handle that.\n",
    "    # For now, let's assume we need to call a generation function.\n",
    "    # The `sample_step` in TimeSeriesDiffusionModel takes `batch` and `batch_idx`\n",
    "    # and seems to append to `self.sample_outputs`.\n",
    "    # To get samples for the perturbed data without altering `model.sample_outputs` from original runs,\n",
    "    # we might need to call a more direct sampling/prediction method of the model if available,\n",
    "    # or temporarily store and then restore `model.sample_outputs`.\n",
    "\n",
    "    # Let's try to get new samples.\n",
    "    # The model's `predict_step` or a similar generation function is needed.\n",
    "    # In TimeSeriesDiffusionModel, `sample_step` is used during `test_step` and it appends to `self.sample_outputs`.\n",
    "    # A more direct way to get samples would be to call `model.model.sample()` (for the inner diffusion model)\n",
    "    # or `model.cond_pred_model.predict()` if it's about conditional prediction.\n",
    "    # Given the existing notebook structure, `model.sample_step` is what was used to generate `model.sample_outputs`.\n",
    "\n",
    "    # To avoid confusion with previously stored sample_outputs, we will call a direct sampling method\n",
    "    # of the underlying diffusion model if possible.\n",
    "    # The TimeSeriesDiffusionModel has a `sample` method.\n",
    "    # Signature: sample(self, batch_x, batch_x_mark, batch_y_mark, N=50, cond_scale=0.)\n",
    "    \n",
    "    # We need to get cond from batch_cov_perturbed\n",
    "    # The model has `self.cond_pred_model.encode_cond(batch_cov)`\n",
    "    # And then uses this `cond` in its own `sample` method, which calls `self.model.sample`.\n",
    "    \n",
    "    model.eval() # Ensure model is in eval mode\n",
    "    with torch.no_grad():\n",
    "        # 1. Encode covariates to get the condition\n",
    "        # The `encode_cond` method might need the batch_cov on the correct device\n",
    "        device = batch_x_orig.device # Assuming batch_x_orig is already on the correct device\n",
    "        \n",
    "        # The covariates in batch_cov_perturbed need to be on the same device as the model\n",
    "        # Typically, the data loader handles this. Here we do it manually if needed.\n",
    "        # Assuming batch_cov_perturbed.feat_dict[stype.numerical] is a tensor.\n",
    "        \n",
    "        # Create a new batch structure for the perturbed data\n",
    "        perturbed_batch_for_sampling = [\n",
    "            (batch_x_orig.to(device), batch_y_orig.to(device), batch_x_mark_orig.to(device), batch_y_mark_orig.to(device)), # Original x, y, x_mark, y_mark\n",
    "            batch_cov_perturbed # Perturbed covariates\n",
    "        ]\n",
    "\n",
    "        model.sample_step(perturbed_batch_for_sampling , 0)\n",
    "    \n",
    "    sampled_output_after_perturb = model.sample_outputs[-1]['pred']\n",
    "\n",
    "    print(f\"Shape of original sampled output (glucose only): {sampled_output_before_perturb.shape}\")\n",
    "    print(f\"Shape of perturbed sampled output (glucose only): {sampled_output_after_perturb.shape}\")\n",
    "\n",
    "    # Store HBA1c values for individuals of interest for plotting/stats\n",
    "    original_hba1c_values = {}\n",
    "    perturbed_hba1c_values = {}\n",
    "\n",
    "    for i_idx in individuals_to_perturb_indices:\n",
    "        original_hba1c_values[i_idx] = batch_cov_orig.feat_dict[stype.numerical][i_idx, 3].item()\n",
    "        perturbed_hba1c_values[i_idx] = batch_cov_perturbed.feat_dict[stype.numerical][i_idx, 3].item()\n",
    "        print(f\"Individual {i_idx}: Original HbA1c: {original_hba1c_values[i_idx]:.2f}, Perturbed HbA1c: {perturbed_hba1c_values[i_idx]:.2f}\")\n",
    "\n",
    "# Ground truth for plotting (from the original selected batch)\n",
    "groundtruth_for_plot = original_batch_data_dict['true'][..., -1]\n",
    "batch_x_for_plot = original_batch_data_dict['batch_x'][..., -1]\n",
    "sampled_output_after_perturb = model.sample_outputs[-1]['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if not model.sample_outputs or 'individuals_to_perturb_indices' not in locals():\n",
    "    print(\"Error: Ensure the perturbation analysis cell has been run and required variables are available.\")\n",
    "else:\n",
    "    # Assuming c_out was 1 or f_dim correctly selected the single glucose feature for pred_len output\n",
    "    # sampled_output_before_perturb shape: (batch_size, num_samples, pred_len)\n",
    "    # sampled_output_after_perturb shape: (batch_size, num_samples, pred_len, 1) from notebook output\n",
    "    # groundtruth_for_plot shape: (batch_size, pred_len)\n",
    "    # batch_x_for_plot shape: (batch_size, seq_len)\n",
    "    \n",
    "    seq_len = batch_x_for_plot.shape[1]\n",
    "    pred_len = groundtruth_for_plot.shape[1]\n",
    "    time_history = np.arange(seq_len)\n",
    "    time_pred = np.arange(seq_len, seq_len + pred_len)\n",
    "    \n",
    "    for idx in individuals_to_perturb_indices:\n",
    "        history_data = batch_x_for_plot[idx].cpu().numpy()\n",
    "        true_future_data = groundtruth_for_plot[idx]\n",
    "        \n",
    "        # Predictions before perturbation\n",
    "        preds_before_raw = sampled_output_before_perturb[idx] # (num_samples, pred_len)\n",
    "        mean_preds_before = np.mean(preds_before_raw, axis=0)\n",
    "        std_preds_before = np.std(preds_before_raw, axis=0)\n",
    "        \n",
    "        # Predictions after perturbation\n",
    "        # sampled_output_after_perturb has shape (batch_size, num_samples, pred_len, 1)\n",
    "        preds_after_raw = sampled_output_after_perturb[idx, ..., 0] # (num_samples, pred_len)\n",
    "        mean_preds_after = np.mean(preds_after_raw, axis=0)\n",
    "        std_preds_after = np.std(preds_after_raw, axis=0)\n",
    "        \n",
    "        plt.figure(figsize=(15, 7))\n",
    "        \n",
    "        # Plot history\n",
    "        plt.plot(time_history, history_data, label='Input History (Glucose)', color='black', linewidth=1.5)\n",
    "        \n",
    "        # Plot true future\n",
    "        plt.plot(time_pred, true_future_data, label='Ground Truth Future (Glucose)', color='green', linestyle='--', linewidth=2)\n",
    "        \n",
    "        # Plot predictions before perturbation\n",
    "        plt.plot(time_pred, mean_preds_before, \n",
    "                 label=f'Mean Pred (Before Perturb, Orig HbA1c: {original_hba1c_values[idx]:.2f})', \n",
    "                 color='blue', linewidth=1.5)\n",
    "        plt.fill_between(time_pred, mean_preds_before - std_preds_before, mean_preds_before + std_preds_before, \n",
    "                         color='blue', alpha=0.2, label='Std Dev (Before)')\n",
    "        \n",
    "        # Plot predictions after perturbation\n",
    "        plt.plot(time_pred, mean_preds_after, \n",
    "                 label=f'Mean Pred (After Perturb, New HbA1c: {perturbed_hba1c_values[idx]:.2f})', \n",
    "                 color='red', linewidth=1.5)\n",
    "        plt.fill_between(time_pred, mean_preds_after - std_preds_after, mean_preds_after + std_preds_after, \n",
    "                         color='red', alpha=0.2, label='Std Dev (After)')\n",
    "        \n",
    "        plt.title(f'Glucose Prediction Perturbation Analysis for Individual {idx}', fontsize=16)\n",
    "        plt.xlabel('Time Steps', fontsize=14)\n",
    "        plt.ylabel('Glucose Value', fontsize=14)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if 'individuals_to_perturb_indices' not in locals() or \\\n",
    "   'sampled_output_before_perturb' not in locals() or \\\n",
    "   'sampled_output_after_perturb' not in locals():\n",
    "    print(\"Error: Ensure the perturbation analysis and plotting cells have been run, and variables are available.\")\n",
    "else:\n",
    "    print(\"\\n--- Comparison of Average Standard Deviations (Time-Averaged) ---\\n\")\n",
    "    avg_std_devs_before_list = []\n",
    "    avg_std_devs_after_list = []\n",
    "\n",
    "    avg_std_devs_before_list = []\n",
    "    avg_std_devs_after_list = []\n",
    "    avg_means_before_list = []\n",
    "    avg_means_after_list = []\n",
    "\n",
    "    for idx in individuals_to_perturb_indices:\n",
    "        # Predictions before perturbation: shape (num_samples, pred_len)\n",
    "        preds_before_raw = sampled_output_before_perturb[idx]\n",
    "        # Std dev across samples for each time step: shape (pred_len,)\n",
    "        std_dev_over_samples_before = np.std(preds_before_raw, axis=0)\n",
    "        # Mean across samples for each time step: shape (pred_len,)\n",
    "        mean_over_samples_before = np.mean(preds_before_raw, axis=0)\n",
    "        # Average this std dev and mean over the prediction length\n",
    "        avg_std_before = np.mean(std_dev_over_samples_before)\n",
    "        avg_mean_before = np.mean(mean_over_samples_before)\n",
    "        avg_std_devs_before_list.append(avg_std_before)\n",
    "        avg_means_before_list.append(avg_mean_before)\n",
    "\n",
    "        # Predictions after perturbation: shape (num_samples, pred_len) after [..., 0] slicing\n",
    "        preds_after_raw = sampled_output_after_perturb[idx, ..., 0]\n",
    "        # Std dev across samples for each time step: shape (pred_len,)\n",
    "        std_dev_over_samples_after = np.std(preds_after_raw, axis=0)\n",
    "        # Mean across samples for each time step: shape (pred_len,)\n",
    "        mean_over_samples_after = np.mean(preds_after_raw, axis=0)\n",
    "        # Average this std dev and mean over the prediction length\n",
    "        avg_std_after = np.mean(std_dev_over_samples_after)\n",
    "        avg_mean_after = np.mean(mean_over_samples_after)\n",
    "        avg_std_devs_after_list.append(avg_std_after)\n",
    "        avg_means_after_list.append(avg_mean_after)\n",
    "\n",
    "        print(f\"Individual {idx}:\")\n",
    "        print(f\"  Avg. Std. Dev (Before Perturbation): {avg_std_before:.4f}\")\n",
    "        print(f\"  Avg. Std. Dev (After Perturbation):  {avg_std_after:.4f}\")\n",
    "        print(f\"  Avg. Mean (Before Perturbation):    {avg_mean_before:.4f}\")\n",
    "        print(f\"  Avg. Mean (After Perturbation):     {avg_mean_after:.4f}\")\n",
    "        if avg_std_after > avg_std_before:\n",
    "            print(f\"  Comparison: Uncertainty (std dev) INCREASED by {avg_std_after - avg_std_before:.4f} after perturbation.\")\n",
    "        elif avg_std_after < avg_std_before:\n",
    "            print(f\"  Comparison: Uncertainty (std dev) DECREASED by {avg_std_before - avg_std_after:.4f} after perturbation.\")\n",
    "        else:\n",
    "            print(f\"  Comparison: Uncertainty (std dev) remained the same after perturbation.\")\n",
    "        print(\"-----\")\n",
    "\n",
    "    # Overall average if desired\n",
    "    if avg_std_devs_before_list and avg_std_devs_after_list and avg_means_before_list and avg_means_after_list:\n",
    "        overall_avg_std_before = np.mean(avg_std_devs_before_list)\n",
    "        overall_avg_std_after = np.mean(avg_std_devs_after_list)\n",
    "        overall_avg_mean_before = np.mean(avg_means_before_list)\n",
    "        overall_avg_mean_after = np.mean(avg_means_after_list)\n",
    "        print(\"\\nOverall Average Across Perturbed Individuals:\")\n",
    "        print(f\"  Overall Avg. Std. Dev (Before): {overall_avg_std_before:.4f}\")\n",
    "        print(f\"  Overall Avg. Std. Dev (After):  {overall_avg_std_after:.4f}\")\n",
    "        print(f\"  Overall Avg. Mean (Before):     {overall_avg_mean_before:.4f}\")\n",
    "        print(f\"  Overall Avg. Mean (After):      {overall_avg_mean_after:.4f}\")\n",
    "        if overall_avg_std_after > overall_avg_std_before:\n",
    "            print(f\"  Overall: Uncertainty INCREASED by {overall_avg_std_after - overall_avg_std_before:.4f}\")\n",
    "        elif overall_avg_std_after < overall_avg_std_before:\n",
    "            print(f\"  Overall: Uncertainty DECREASED by {overall_avg_std_before - overall_avg_std_after:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Overall: Uncertainty remained the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_steps_batch_x(original_data_tuple, individual_indices, percentage_increase):\n",
    "    \"\"\"\n",
    "    Perturbs 'steps' for specified individuals in batch_x.\n",
    "    'steps' are derived from batch_x[:, :, STEPS_FEATURE_INDEX_IN_BATCH_X] using a specific formula.\n",
    "    This function assumes batch_x is the first element of original_data_tuple:\n",
    "    original_data_tuple = (batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "    Returns a new data_tuple with batch_x perturbed; does not modify the input tuple or its tensors.\n",
    "    \"\"\"\n",
    "    # Constants for step calculation, as provided\n",
    "    STEPS_SCALE = 20.84327263\n",
    "    STEPS_OFFSET = 6.53019535e+00\n",
    "    STEPS_FEATURE_INDEX_IN_BATCH_X = 1 # 0-indexed\n",
    "\n",
    "\n",
    "    original_batch_x = original_data_tuple[0]\n",
    "    perturbed_batch_x = original_batch_x.clone() # Ensure we don't modify the original tensor\n",
    "\n",
    "    for idx in individual_indices:\n",
    "        # Extract the scaled feature series for steps for the specific individual\n",
    "        scaled_steps_series = perturbed_batch_x[idx, :, STEPS_FEATURE_INDEX_IN_BATCH_X]\n",
    "\n",
    "        # Calculate current \"true\" step values (element-wise for the series)\n",
    "        current_true_steps = scaled_steps_series * STEPS_SCALE + STEPS_OFFSET\n",
    "\n",
    "        # Perturb the \"true\" step values\n",
    "        perturbed_true_steps = current_true_steps * 0\n",
    "        # Convert perturbed \"true\" steps back to scaled values for storage in batch_x\n",
    "        new_scaled_steps_series = (perturbed_true_steps - STEPS_OFFSET) / STEPS_SCALE\n",
    "\n",
    "        # Update the cloned batch_x with the new scaled step series\n",
    "        perturbed_batch_x[idx, :, STEPS_FEATURE_INDEX_IN_BATCH_X] = new_scaled_steps_series\n",
    "\n",
    "\n",
    "    # Reconstruct the data tuple with the perturbed batch_x\n",
    "    new_data_list = list(original_data_tuple)\n",
    "    new_data_list[0] = perturbed_batch_x\n",
    "    new_data_tuple = tuple(new_data_list)\n",
    "    \n",
    "    return new_data_tuple\n",
    "\n",
    "def perturb_steps_and_hr_batch_x(original_data_tuple, individual_indices, percentage_increase, time_steps_to_perturb_start, time_steps_to_perturb_end):\n",
    "    \"\"\"\n",
    "    Perturbs both steps AND heart rate together to maintain physiological correlation\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    STEPS_SCALE = 20.84327263\n",
    "    STEPS_OFFSET = 6.53019535e+00\n",
    "    STEPS_FEATURE_INDEX = 1\n",
    "    \n",
    "    HR_SCALE = 79.3461185\n",
    "    HR_OFFSET = 20.41707644  \n",
    "    HR_FEATURE_INDEX = 0\n",
    "    \n",
    "    original_batch_x = original_data_tuple[0]\n",
    "    perturbed_batch_x = original_batch_x.clone()\n",
    "\n",
    "    for idx in individual_indices:\n",
    "        # Perturb steps\n",
    "        scaled_steps = perturbed_batch_x[idx, time_steps_to_perturb_start:time_steps_to_perturb_end, STEPS_FEATURE_INDEX]\n",
    "        true_steps = scaled_steps * STEPS_SCALE + STEPS_OFFSET\n",
    "\n",
    "        perturbed_true_steps = true_steps * (1 + percentage_increase / 100.0)\n",
    "        new_scaled_steps = (perturbed_true_steps - STEPS_OFFSET) / STEPS_SCALE\n",
    "        perturbed_batch_x[idx, time_steps_to_perturb_start:time_steps_to_perturb_end, STEPS_FEATURE_INDEX] = new_scaled_steps\n",
    "        \n",
    "        # Perturb heart rate proportionally (maybe smaller increase, e.g., 50% of steps increase)\n",
    "        scaled_hr = perturbed_batch_x[idx, time_steps_to_perturb_start:time_steps_to_perturb_end, HR_FEATURE_INDEX]\n",
    "        true_hr = scaled_hr * HR_SCALE + HR_OFFSET\n",
    "        # fit a linear model to the hr and steps and use this to calculate the hr_percentage_increase\n",
    "        # Fit a linear model (least squares) between true_steps and true_hr for this individual\n",
    "        # Only use nonzero steps to avoid spurious correlation with rest periods\n",
    "        import numpy as np\n",
    "\n",
    "        steps_np = true_steps.cpu().numpy() if hasattr(true_steps, \"cpu\") else true_steps.numpy()\n",
    "        hr_np = true_hr.cpu().numpy() if hasattr(true_hr, \"cpu\") else true_hr.numpy()\n",
    "        nonzero_mask = steps_np > 0\n",
    "\n",
    "        if np.sum(nonzero_mask) > 1:\n",
    "            # Fit: hr = a * steps + b\n",
    "            a, b = np.polyfit(steps_np[nonzero_mask], hr_np[nonzero_mask], 1)\n",
    "            # Predict new HR increase based on the increase in steps\n",
    "            avg_steps = np.mean(steps_np[nonzero_mask])\n",
    "            avg_hr = np.mean(hr_np[nonzero_mask])\n",
    "            # Estimate expected HR increase for the given steps increase\n",
    "            expected_hr_increase = a * avg_steps * (percentage_increase / 100.0)\n",
    "            hr_percentage_increase = (expected_hr_increase / avg_hr) * 100 if avg_hr != 0 else percentage_increase * 0.5\n",
    "        else:\n",
    "            hr_percentage_increase = percentage_increase * 0.5  # fallback if not enough data\n",
    "        \n",
    "        hr_percentage_increase = percentage_increase * 0.5  # Adjust this ratio as needed\n",
    "        perturbed_true_hr = true_hr * (1 + hr_percentage_increase / 100.0)\n",
    "        # perturbed_true_hr[~true_steps_zero_mask] = true_hr[~true_steps_zero_mask] * (1 + 30 / 100.0)\n",
    "        new_scaled_hr = (perturbed_true_hr - HR_OFFSET) / HR_SCALE\n",
    "        perturbed_batch_x[idx, time_steps_to_perturb_start:time_steps_to_perturb_end, HR_FEATURE_INDEX] = new_scaled_hr\n",
    "\n",
    "    # Reconstruct tuple\n",
    "    new_data_list = list(original_data_tuple)\n",
    "    new_data_list[0] = perturbed_batch_x\n",
    "    return tuple(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps Perturbation Analysis\n",
    "# Using the last batch from model.sample_outputs for perturbation analysis\n",
    "model.cuda()\n",
    "if not model.sample_outputs:\n",
    "    print(\"Error: model.sample_outputs is empty. Please ensure the model has processed data.\")\n",
    "else:\n",
    "    # Use the last batch for perturbation\n",
    "    batch_index_to_perturb = 0\n",
    "    original_batch_data_dict = model.sample_outputs[batch_index_to_perturb]\n",
    "    original_batch_tuple = original_batch_data_dict['batch']\n",
    "    \n",
    "    batch_x_orig, batch_y_orig, batch_x_mark_orig, batch_y_mark_orig = original_batch_tuple[0]\n",
    "    batch_cov_orig = original_batch_tuple[1]\n",
    "    \n",
    "    # Get original predictions before perturbation\n",
    "    sampled_output_before_perturb = original_batch_data_dict['pred'][..., -1]  # Glucose channel only\n",
    "    \n",
    "    num_individuals_in_batch = batch_x_orig.shape[0]\n",
    "    num_to_perturb = min(256, num_individuals_in_batch)\n",
    "    \n",
    "    # Randomly select individuals to perturb\n",
    "    individuals_to_perturb_indices = random.sample(range(num_individuals_in_batch), num_to_perturb)\n",
    "    print(f\"Original batch size: {num_individuals_in_batch}\")\n",
    "    print(f\"Randomly selected individuals to perturb (indices): {individuals_to_perturb_indices}\")\n",
    "    \n",
    "    # Apply steps perturbation with 50% increase\n",
    "    percentage_increase_steps = 300\n",
    "    perturbed_batch_tuple = perturb_steps_and_hr_batch_x(original_batch_tuple[0], individuals_to_perturb_indices, percentage_increase_steps, 0,72)\n",
    "    \n",
    "    # Calculate original and perturbed steps values for comparison\n",
    "    STEPS_SCALE = 20.84327263\n",
    "    STEPS_OFFSET = 6.53019535e+00\n",
    "    STEPS_FEATURE_INDEX_IN_BATCH_X = 1\n",
    "    \n",
    "    original_steps_values = {}\n",
    "    perturbed_steps_values = {}\n",
    "    \n",
    "    for i_idx in individuals_to_perturb_indices:\n",
    "        # Calculate original steps (average across time series)\n",
    "        orig_scaled = batch_x_orig[i_idx, :, STEPS_FEATURE_INDEX_IN_BATCH_X].mean().item()\n",
    "        orig_true_steps = orig_scaled * STEPS_SCALE + STEPS_OFFSET\n",
    "        \n",
    "        # Calculate perturbed steps\n",
    "        pert_scaled = perturbed_batch_tuple[0][i_idx, :, STEPS_FEATURE_INDEX_IN_BATCH_X].mean().item()\n",
    "        pert_true_steps = pert_scaled * STEPS_SCALE + STEPS_OFFSET\n",
    "        \n",
    "        original_steps_values[i_idx] = orig_true_steps\n",
    "        perturbed_steps_values[i_idx] = pert_true_steps\n",
    "        \n",
    "        print(f\"Individual {i_idx}: Original Steps: {orig_true_steps:.2f}, Perturbed Steps: {pert_true_steps:.2f}\")\n",
    "    \n",
    "    # Create perturbed batch for model inference\n",
    "    perturbed_batch_for_sampling = [\n",
    "        perturbed_batch_tuple,\n",
    "        batch_cov_orig\n",
    "    ]\n",
    "    \n",
    "    # Run model inference with perturbed data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.sample_step(perturbed_batch_for_sampling, 1)\n",
    "    \n",
    "    # Get perturbed predictions\n",
    "    sampled_output_after_perturb = model.sample_outputs[-1]['pred'][..., -1]\n",
    "    \n",
    "    print(f\"\\nShape of original sampled output (glucose only): {sampled_output_before_perturb.shape}\")\n",
    "    print(f\"Shape of perturbed sampled output (glucose only): {sampled_output_after_perturb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation on Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and Statistical Analysis of Steps Perturbation Results\n",
    "\n",
    "# Calculate prediction statistics\n",
    "pred_mean_before = sampled_output_before_perturb.mean(axis=1)  # Mean across samples\n",
    "pred_std_before = sampled_output_before_perturb.std(axis=1)    # Std across samples\n",
    "pred_mean_after = sampled_output_after_perturb.mean(axis=1)   # Mean across samples  \n",
    "pred_std_after = sampled_output_after_perturb.std(axis=1)     # Std across samples\n",
    "\n",
    "# Print individual statistics for perturbed individuals\n",
    "print(\"\\n=== Individual Statistics ===\")\n",
    "for i_idx in individuals_to_perturb_indices:\n",
    "    print(f\"\\nIndividual {i_idx}:\")\n",
    "    print(f\"  Steps: {original_steps_values[i_idx]:.2f} ‚Üí {perturbed_steps_values[i_idx]:.2f} ({percentage_increase_steps}% increase)\")\n",
    "    print(f\"  Pred Mean: {pred_mean_before[i_idx].mean():.3f} ‚Üí {pred_mean_after[i_idx].mean():.3f}\")\n",
    "    print(f\"  Pred Std:  {pred_std_before[i_idx].mean():.3f} ‚Üí {pred_std_after[i_idx].mean():.3f}\")\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\n=== Overall Statistics ===\")\n",
    "print(f\"Overall prediction mean change: {pred_mean_before.mean():.3f} ‚Üí {pred_mean_after.mean():.3f}\")\n",
    "print(f\"Overall prediction std change:  {pred_std_before.mean():.3f} ‚Üí {pred_std_after.mean():.3f}\")\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Steps Perturbation Analysis Results', fontsize=16)\n",
    "\n",
    "# Plot 1: Individual glucose predictions before/after for selected individuals\n",
    "ax1 = axes[0, 0]\n",
    "n_individuals_to_plot = min(3, len(individuals_to_perturb_indices))\n",
    "for plot_idx, i_idx in enumerate(individuals_to_perturb_indices[:n_individuals_to_plot]):\n",
    "    time_steps = range(pred_mean_before.shape[1])\n",
    "    ax1.plot(time_steps, pred_mean_before[i_idx], 'b-', alpha=0.7, label=f'Before (Ind {i_idx})' if plot_idx == 0 else \"\")\n",
    "    ax1.plot(time_steps, pred_mean_after[i_idx], 'r--', alpha=0.7, label=f'After (Ind {i_idx})' if plot_idx == 0 else \"\")\n",
    "\n",
    "ax1.set_title('Individual Glucose Predictions')\n",
    "ax1.set_xlabel('Time Steps')\n",
    "ax1.set_ylabel('Glucose Level')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Average prediction change across all perturbed individuals\n",
    "ax2 = axes[0, 1]\n",
    "perturbed_before = pred_mean_before[individuals_to_perturb_indices]\n",
    "perturbed_after = pred_mean_after[individuals_to_perturb_indices]\n",
    "time_steps = range(pred_mean_before.shape[1])\n",
    "ax2.plot(time_steps, perturbed_before.mean(axis=0), 'b-', linewidth=2, label='Before Perturbation')\n",
    "ax2.plot(time_steps, perturbed_after.mean(axis=0), 'r-', linewidth=2, label='After Perturbation')\n",
    "ax2.fill_between(time_steps, \n",
    "                 perturbed_before.mean(axis=0) - perturbed_before.std(axis=0),\n",
    "                 perturbed_before.mean(axis=0) + perturbed_before.std(axis=0),\n",
    "                 alpha=0.2, color='blue')\n",
    "ax2.fill_between(time_steps,\n",
    "                 perturbed_after.mean(axis=0) - perturbed_after.std(axis=0), \n",
    "                 perturbed_after.mean(axis=0) + perturbed_after.std(axis=0),\n",
    "                 alpha=0.2, color='red')\n",
    "ax2.set_title('Average Glucose Predictions (Perturbed Individuals)')\n",
    "ax2.set_xlabel('Time Steps')\n",
    "ax2.set_ylabel('Glucose Level')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Steps values comparison\n",
    "ax3 = axes[1, 0]\n",
    "individuals_plot = list(individuals_to_perturb_indices[:5])  # Show first 5\n",
    "orig_steps = [original_steps_values[i] for i in individuals_plot]\n",
    "pert_steps = [perturbed_steps_values[i] for i in individuals_plot]\n",
    "x_pos = range(len(individuals_plot))\n",
    "width = 0.35\n",
    "ax3.bar([x - width/2 for x in x_pos], orig_steps, width, label='Original Steps', alpha=0.7)\n",
    "ax3.bar([x + width/2 for x in x_pos], pert_steps, width, label='Perturbed Steps', alpha=0.7)\n",
    "ax3.set_title('Steps Values: Before vs After Perturbation')\n",
    "ax3.set_xlabel('Individual Index')\n",
    "ax3.set_ylabel('Steps Value')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels([f'Ind {i}' for i in individuals_plot])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Prediction difference distribution\n",
    "ax4 = axes[1, 1]\n",
    "pred_diff = pred_mean_after - pred_mean_before\n",
    "perturbed_diff = pred_diff[individuals_to_perturb_indices].flatten()\n",
    "control_diff = np.delete(pred_diff, individuals_to_perturb_indices, axis=0).flatten()\n",
    "ax4.hist(perturbed_diff, bins=30, alpha=0.7, label='Perturbed Individuals', color='red')\n",
    "ax4.hist(control_diff, bins=30, alpha=0.7, label='Control Individuals', color='blue')\n",
    "ax4.set_title('Distribution of Prediction Changes')\n",
    "ax4.set_xlabel('Glucose Prediction Change')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary analysis\n",
    "print(f\"\\n=== Summary Analysis ===\")\n",
    "print(f\"Average steps increase applied: {percentage_increase_steps}%\")\n",
    "print(f\"Number of individuals perturbed: {len(individuals_to_perturb_indices)}\")\n",
    "mean_pred_change_perturbed = pred_diff[individuals_to_perturb_indices].mean()\n",
    "mean_pred_change_control = np.delete(pred_diff, individuals_to_perturb_indices, axis=0).mean()\n",
    "print(f\"Average glucose prediction change (perturbed): {mean_pred_change_perturbed:.4f}\")\n",
    "print(f\"Average glucose prediction change (control): {mean_pred_change_control:.4f}\")\n",
    "print(f\"Differential effect: {mean_pred_change_perturbed - mean_pred_change_control:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Individual time series plots for each perturbed individual (following HbA1c style)\n",
    "if 'individuals_to_perturb_indices' not in locals() or \\\n",
    "   'sampled_output_before_perturb' not in locals() or \\\n",
    "   'sampled_output_after_perturb' not in locals():\n",
    "    print(\"Error: Ensure the perturbation analysis has been run and variables are available.\")\n",
    "else:\n",
    "    # Get the time series data for plotting\n",
    "    seq_len = batch_x_orig.shape[1]\n",
    "    # Defensive: pred_len should be the last dimension of the predictions\n",
    "    # For before perturb: shape (num_individuals, num_samples, pred_len)\n",
    "    # For after perturb: shape (num_individuals, num_samples, pred_len, num_channels) or (num_individuals, num_samples, pred_len) if squeezed\n",
    "    # We'll handle both cases\n",
    "    if sampled_output_before_perturb.ndim == 3:\n",
    "        pred_len = sampled_output_before_perturb.shape[2]\n",
    "    elif sampled_output_before_perturb.ndim == 2:\n",
    "        pred_len = sampled_output_before_perturb.shape[1]\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected shape for sampled_output_before_perturb\")\n",
    "    time_history = np.arange(seq_len)\n",
    "    time_pred = np.arange(seq_len, seq_len + pred_len)\n",
    "\n",
    "    # Plot individual time series for each perturbed individual\n",
    "    for idx in individuals_to_perturb_indices:\n",
    "        # Get input history (glucose channel)\n",
    "        history_data = batch_x_orig[idx, :, 0].cpu().numpy()  # Assuming glucose is channel 0\n",
    "\n",
    "        # Get ground truth future\n",
    "        true_future_data = batch_y_orig[idx, -pred_len:, 0].cpu().numpy()  # Glucose channel\n",
    "\n",
    "        # Get predictions before perturbation\n",
    "        preds_before_raw = sampled_output_before_perturb[idx]  # (num_samples, pred_len)\n",
    "        if preds_before_raw.ndim == 2:\n",
    "            mean_preds_before = np.mean(preds_before_raw, axis=0)\n",
    "            std_preds_before = np.std(preds_before_raw, axis=0)\n",
    "        elif preds_before_raw.ndim == 1:\n",
    "            mean_preds_before = preds_before_raw\n",
    "            std_preds_before = np.zeros_like(mean_preds_before)\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected shape for preds_before_raw\")\n",
    "\n",
    "        # Get predictions after perturbation\n",
    "        preds_after_raw = sampled_output_after_perturb[idx]\n",
    "        # Handle possible extra channel dimension\n",
    "        if preds_after_raw.ndim == 3:\n",
    "            # (num_samples, pred_len, num_channels)\n",
    "            preds_after_raw = preds_after_raw[..., 0]  # Glucose channel\n",
    "        if preds_after_raw.ndim == 2:\n",
    "            mean_preds_after = np.mean(preds_after_raw, axis=0)\n",
    "            std_preds_after = np.std(preds_after_raw, axis=0)\n",
    "        elif preds_after_raw.ndim == 1:\n",
    "            mean_preds_after = preds_after_raw\n",
    "            std_preds_after = np.zeros_like(mean_preds_after)\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected shape for preds_after_raw\")\n",
    "\n",
    "        # Ensure all arrays are 1D and of length pred_len\n",
    "        mean_preds_before = np.asarray(mean_preds_before).flatten()\n",
    "        std_preds_before = np.asarray(std_preds_before).flatten()\n",
    "        mean_preds_after = np.asarray(mean_preds_after).flatten()\n",
    "        std_preds_after = np.asarray(std_preds_after).flatten()\n",
    "        true_future_data = np.asarray(true_future_data).flatten()\n",
    "        # Defensive: truncate or pad to pred_len if needed\n",
    "        if mean_preds_before.shape[0] != pred_len:\n",
    "            mean_preds_before = mean_preds_before[:pred_len]\n",
    "            std_preds_before = std_preds_before[:pred_len]\n",
    "        if mean_preds_after.shape[0] != pred_len:\n",
    "            mean_preds_after = mean_preds_after[:pred_len]\n",
    "            std_preds_after = std_preds_after[:pred_len]\n",
    "        if true_future_data.shape[0] != pred_len:\n",
    "            true_future_data = true_future_data[:pred_len]\n",
    "\n",
    "        plt.figure(figsize=(15, 7))\n",
    "\n",
    "        # Plot input history\n",
    "        plt.plot(time_history, history_data, label='Input History (Glucose)',\n",
    "                 color='black', linewidth=1.5)\n",
    "\n",
    "        # Plot ground truth future\n",
    "        plt.plot(time_pred, true_future_data, label='Ground Truth Future (Glucose)',\n",
    "                 color='green', linestyle='--', linewidth=2)\n",
    "\n",
    "        # Plot predictions before perturbation\n",
    "        plt.plot(time_pred, mean_preds_before,\n",
    "                 label=f'Mean Pred (Before Perturb, Orig Steps: {original_steps_values[idx]:.1f})',\n",
    "                 color='blue', linewidth=1.5)\n",
    "        plt.fill_between(time_pred, mean_preds_before - std_preds_before,\n",
    "                         mean_preds_before + std_preds_before,\n",
    "                         color='blue', alpha=0.2, label='Std Dev (Before)')\n",
    "\n",
    "        # Plot predictions after perturbation\n",
    "        plt.plot(time_pred, mean_preds_after,\n",
    "                 label=f'Mean Pred (After Perturb, New Steps: {perturbed_steps_values[idx]:.1f})',\n",
    "                 color='red', linewidth=1.5)\n",
    "        plt.fill_between(time_pred, mean_preds_after - std_preds_after,\n",
    "                         mean_preds_after + std_preds_after,\n",
    "                         color='red', alpha=0.2, label='Std Dev (After)')\n",
    "\n",
    "        plt.title(f'Glucose Prediction Steps Perturbation Analysis for Individual {idx}', fontsize=16)\n",
    "        plt.xlabel('Time Steps', fontsize=14)\n",
    "        plt.ylabel('Glucose Value', fontsize=14)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# === Summary statistics analysis ====================================\n",
    "# ---------------------------------------------------------------------\n",
    "print(f\"\\n=== Steps Perturbation Summary Analysis ===\")\n",
    "print(f\"Perturbation applied: {percentage_increase_steps}% change in steps\")\n",
    "print(f\"Number of individuals perturbed: {len(individuals_to_perturb_indices)}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1.  Prediction means (¬µ) and 95 %-CI half-widths (¬±1.96¬∑œÉ/‚àön)\n",
    "# ---------------------------------------------------------------------\n",
    "# -- mean predictions -------------------------------------------------\n",
    "pred_mean_before = sampled_output_before_perturb.mean(axis=1)            # (N, pred_len)\n",
    "\n",
    "# unify AFTER tensor shape to (N, n_samples, pred_len)\n",
    "sampled_after = (\n",
    "    sampled_output_after_perturb[..., 0] if sampled_output_after_perturb.ndim == 4\n",
    "    else sampled_output_after_perturb\n",
    ")\n",
    "pred_mean_after = sampled_after.mean(axis=1)                              # (N, pred_len)\n",
    "\n",
    "# -- 95 %-CI half-widths ---------------------------------------------\n",
    "n_samp_b, n_samp_a = sampled_output_before_perturb.shape[1], sampled_after.shape[1]\n",
    "ci_half_before = 1.96 * sampled_output_before_perturb.std(axis=1) / np.sqrt(n_samp_b)\n",
    "ci_half_after  = 1.96 * sampled_after.std(axis=1)              / np.sqrt(n_samp_a)\n",
    "\n",
    "# average CI change (after - before)  ‚ûú report *width* change\n",
    "ci_change      = ci_half_after - ci_half_before                 # (N, pred_len)\n",
    "pert_ci_change = ci_change[individuals_to_perturb_indices].mean()\n",
    "ctrl_indices   = [i for i in range(ci_change.shape[0]) if i not in individuals_to_perturb_indices]\n",
    "ctrl_ci_change = ci_change[ctrl_indices].mean() if ctrl_indices else 0.0\n",
    "\n",
    "print(f\"Average 95 %-CI width change (perturbed individuals): {pert_ci_change:+.4f}\")\n",
    "print(f\"Average 95 %-CI width change (control individuals)  : {ctrl_ci_change:+.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2.  Point estimate differences --------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "pred_diff      = pred_mean_after - pred_mean_before                       # (N, pred_len)\n",
    "pert_diff      = pred_diff[individuals_to_perturb_indices].mean()\n",
    "ctrl_diff      = pred_diff[ctrl_indices].mean() if ctrl_indices else 0.0\n",
    "\n",
    "print(f\"Average glucose prediction change  (perturbed) : {pert_diff:+.4f}\")\n",
    "print(f\"Average glucose prediction change  (control)   : {ctrl_diff:+.4f}\")\n",
    "print(f\"Differential effect of steps perturbation      : {(pert_diff-ctrl_diff):+.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3.  Per-individual report  (filtered) --------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "print(f\"\\n=== Individual Results (|Œî¬µ| ‚â• 0.1) ===\")\n",
    "for i_idx in individuals_to_perturb_indices:\n",
    "    steps_change = perturbed_steps_values[i_idx] - original_steps_values[i_idx]\n",
    "    # use last-timestep mean prediction\n",
    "    pred_change  = pred_mean_after[i_idx, -1] - pred_mean_before[i_idx, -1]\n",
    "    \n",
    "\n",
    "    CI_change = ci_change[i_idx].mean()\n",
    "\n",
    "\n",
    "    # skip tiny effects\n",
    "    if  (abs(steps_change) < 1):\n",
    "        continue\n",
    "\n",
    "    print(f\"Individual {i_idx}:\")\n",
    "    print(f\"  CI change: {CI_change}\")\n",
    "    print(f\"  Steps change: {original_steps_values[i_idx]:.1f} ‚Üí {perturbed_steps_values[i_idx]:.1f} \"\n",
    "          f\"({steps_change:+.1f})\")\n",
    "    print(f\"before pred: {pred_mean_before[i_idx].mean()}\")\n",
    "    print(f\"after pred: {pred_mean_after[i_idx].mean()}\")\n",
    "    print(f\"  Avg glucose prediction change: {pred_change:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Individual time series plots for each perturbed individual (following HbA1c style)\n",
    "if 'individuals_to_perturb_indices' not in locals() or \\\n",
    "   'sampled_output_before_perturb' not in locals() or \\\n",
    "   'sampled_output_after_perturb' not in locals():\n",
    "    print(\"Error: Ensure the perturbation analysis has been run and variables are available.\")\n",
    "else:\n",
    "    # Filter individuals based on step change > 20\n",
    "    significant_change_indices = []\n",
    "    for idx in individuals_to_perturb_indices:\n",
    "        steps_change = abs(perturbed_steps_values[idx] - original_steps_values[idx])\n",
    "        if steps_change > 10:\n",
    "            significant_change_indices.append(idx)\n",
    "    \n",
    "    print(f\"=== Filtering Analysis: Step Changes > 20 ===\")\n",
    "    print(f\"Total perturbed individuals: {len(individuals_to_perturb_indices)}\")\n",
    "    print(f\"Individuals with step change > 20: {len(significant_change_indices)}\")\n",
    "    \n",
    "    if len(significant_change_indices) == 0:\n",
    "        print(\"No individuals have step changes > 20. Cannot proceed with analysis.\")\n",
    "    else:\n",
    "        # Get the time series data for plotting\n",
    "        seq_len = batch_x_orig.shape[1]\n",
    "        # Defensive: pred_len should be the last dimension of the predictions\n",
    "        if sampled_output_before_perturb.ndim == 3:\n",
    "            pred_len = sampled_output_before_perturb.shape[2]\n",
    "        elif sampled_output_before_perturb.ndim == 2:\n",
    "            pred_len = sampled_output_before_perturb.shape[1]\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected shape for sampled_output_before_perturb\")\n",
    "        time_history = np.arange(seq_len)\n",
    "        time_pred = np.arange(seq_len, seq_len + pred_len)\n",
    "\n",
    "        # Plot individual time series for each individual with significant step change\n",
    "        for idx in significant_change_indices:\n",
    "            # Get input history (glucose channel)\n",
    "            history_data = batch_x_orig[idx, :, 0].cpu().numpy()  # Assuming glucose is channel 0\n",
    "\n",
    "            # Get ground truth future\n",
    "            true_future_data = batch_y_orig[idx, -pred_len:, 0].cpu().numpy()  # Glucose channel\n",
    "\n",
    "            # Get predictions before perturbation\n",
    "            preds_before_raw = sampled_output_before_perturb[idx]  # (num_samples, pred_len)\n",
    "            if preds_before_raw.ndim == 2:\n",
    "                mean_preds_before = np.mean(preds_before_raw, axis=0)\n",
    "                std_preds_before = np.std(preds_before_raw, axis=0)\n",
    "                # Calculate 95% confidence intervals\n",
    "                ci_lower_before = np.percentile(preds_before_raw, 2.5, axis=0)\n",
    "                ci_upper_before = np.percentile(preds_before_raw, 97.5, axis=0)\n",
    "            elif preds_before_raw.ndim == 1:\n",
    "                mean_preds_before = preds_before_raw\n",
    "                std_preds_before = np.zeros_like(mean_preds_before)\n",
    "                ci_lower_before = mean_preds_before\n",
    "                ci_upper_before = mean_preds_before\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected shape for preds_before_raw\")\n",
    "\n",
    "            # Get predictions after perturbation\n",
    "            preds_after_raw = sampled_output_after_perturb[idx]\n",
    "            # Handle possible extra channel dimension\n",
    "            if preds_after_raw.ndim == 3:\n",
    "                # (num_samples, pred_len, num_channels)\n",
    "                preds_after_raw = preds_after_raw[..., 0]  # Glucose channel\n",
    "            if preds_after_raw.ndim == 2:\n",
    "                mean_preds_after = np.mean(preds_after_raw, axis=0)\n",
    "                std_preds_after = np.std(preds_after_raw, axis=0)\n",
    "                # Calculate 95% confidence intervals\n",
    "                ci_lower_after = np.percentile(preds_after_raw, 2.5, axis=0)\n",
    "                ci_upper_after = np.percentile(preds_after_raw, 97.5, axis=0)\n",
    "            elif preds_after_raw.ndim == 1:\n",
    "                mean_preds_after = preds_after_raw\n",
    "                std_preds_after = np.zeros_like(mean_preds_after)\n",
    "                ci_lower_after = mean_preds_after\n",
    "                ci_upper_after = mean_preds_after\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected shape for preds_after_raw\")\n",
    "\n",
    "            # Ensure all arrays are 1D and of length pred_len\n",
    "            mean_preds_before = np.asarray(mean_preds_before).flatten()\n",
    "            std_preds_before = np.asarray(std_preds_before).flatten()\n",
    "            ci_lower_before = np.asarray(ci_lower_before).flatten()\n",
    "            ci_upper_before = np.asarray(ci_upper_before).flatten()\n",
    "            \n",
    "            mean_preds_after = np.asarray(mean_preds_after).flatten()\n",
    "            std_preds_after = np.asarray(std_preds_after).flatten()\n",
    "            ci_lower_after = np.asarray(ci_lower_after).flatten()\n",
    "            ci_upper_after = np.asarray(ci_upper_after).flatten()\n",
    "            \n",
    "            true_future_data = np.asarray(true_future_data).flatten()\n",
    "            \n",
    "            # Defensive: truncate or pad to pred_len if needed\n",
    "            if mean_preds_before.shape[0] != pred_len:\n",
    "                mean_preds_before = mean_preds_before[:pred_len]\n",
    "                std_preds_before = std_preds_before[:pred_len]\n",
    "                ci_lower_before = ci_lower_before[:pred_len]\n",
    "                ci_upper_before = ci_upper_before[:pred_len]\n",
    "            if mean_preds_after.shape[0] != pred_len:\n",
    "                mean_preds_after = mean_preds_after[:pred_len]\n",
    "                std_preds_after = std_preds_after[:pred_len]\n",
    "                ci_lower_after = ci_lower_after[:pred_len]\n",
    "                ci_upper_after = ci_upper_after[:pred_len]\n",
    "            if true_future_data.shape[0] != pred_len:\n",
    "                true_future_data = true_future_data[:pred_len]\n",
    "\n",
    "            plt.figure(figsize=(15, 8))\n",
    "\n",
    "            # Plot input history\n",
    "            plt.plot(time_history, history_data, label='Input History (Glucose)',\n",
    "                     color='black', linewidth=1.5)\n",
    "\n",
    "            # Plot ground truth future\n",
    "            plt.plot(time_pred, true_future_data, label='Ground Truth Future (Glucose)',\n",
    "                     color='green', linestyle='--', linewidth=2)\n",
    "\n",
    "            # Plot predictions before perturbation with confidence intervals\n",
    "            plt.plot(time_pred, mean_preds_before,\n",
    "                     label=f'Mean Pred (Before Perturb, Orig Steps: {original_steps_values[idx]:.1f})',\n",
    "                     color='blue', linewidth=1.5)\n",
    "            plt.fill_between(time_pred, ci_lower_before, ci_upper_before,\n",
    "                             color='blue', alpha=0.2, label='95% CI (Before)')\n",
    "\n",
    "            # Plot predictions after perturbation with confidence intervals\n",
    "            plt.plot(time_pred, mean_preds_after,\n",
    "                     label=f'Mean Pred (After Perturb, New Steps: {perturbed_steps_values[idx]:.1f})',\n",
    "                     color='red', linewidth=1.5)\n",
    "            plt.fill_between(time_pred, ci_lower_after, ci_upper_after,\n",
    "                             color='red', alpha=0.2, label='95% CI (After)')\n",
    "\n",
    "            # Calculate step change for title\n",
    "            steps_change = perturbed_steps_values[idx] - original_steps_values[idx]\n",
    "            plt.title(f'Glucose Prediction Steps Perturbation Analysis for Individual {idx}\\n'\n",
    "                     f'Step Change: {steps_change:+.1f} (>{20} threshold)', fontsize=16)\n",
    "            plt.xlabel('Time Steps', fontsize=14)\n",
    "            plt.ylabel('Glucose Value', fontsize=14)\n",
    "            plt.legend(fontsize=10, loc='best')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Summary statistics analysis (only for individuals with step change > 20)\n",
    "        print(f\"\\n=== Steps Perturbation Summary Analysis (Step Change > 20) ===\")\n",
    "        print(f\"Perturbation applied: {percentage_increase_steps}% change in steps\")\n",
    "        print(f\"Number of individuals with significant step change (>20): {len(significant_change_indices)}\")\n",
    "\n",
    "        # Calculate prediction differences (only for significant changes)\n",
    "        pred_mean_before = sampled_output_before_perturb.mean(axis=1)  # (num_individuals, pred_len)\n",
    "        if sampled_output_after_perturb.ndim == 4:\n",
    "            pred_mean_after = sampled_output_after_perturb[..., 0].mean(axis=1)  # (num_individuals, pred_len)\n",
    "        else:\n",
    "            pred_mean_after = sampled_output_after_perturb.mean(axis=1)   # (num_individuals, pred_len)\n",
    "        pred_diff = pred_mean_after - pred_mean_before  # (num_individuals, pred_len)\n",
    "\n",
    "        # Calculate effect sizes (only for individuals with step change > 20)\n",
    "        if len(significant_change_indices) > 0:\n",
    "            significant_diff = pred_diff[significant_change_indices].mean()\n",
    "            \n",
    "            # Control group: individuals not significantly perturbed\n",
    "            control_indices = [i for i in range(pred_diff.shape[0]) if i not in significant_change_indices]\n",
    "            control_diff = pred_diff[control_indices].mean() if control_indices else 0\n",
    "\n",
    "            print(f\"Average glucose prediction change (individuals with step change >20): {significant_diff:.4f}\")\n",
    "            print(f\"Average glucose prediction change (control individuals): {control_diff:.4f}\")\n",
    "            print(f\"Differential effect of significant steps perturbation: {significant_diff - control_diff:.4f}\")\n",
    "\n",
    "            # Individual statistics (only for significant changes)\n",
    "            print(f\"\\n=== Individual Results (Step Change > 20) ===\")\n",
    "            for i_idx in significant_change_indices:\n",
    "                steps_change = perturbed_steps_values[i_idx] - original_steps_values[i_idx]\n",
    "                pred_change = pred_mean_after[i_idx].mean() - pred_mean_before[i_idx].mean()\n",
    "                \n",
    "                # Calculate confidence intervals for the prediction change\n",
    "                preds_before_individual = sampled_output_before_perturb[i_idx]\n",
    "                preds_after_individual = sampled_output_after_perturb[i_idx]\n",
    "                if preds_after_individual.ndim == 3:\n",
    "                    preds_after_individual = preds_after_individual[..., 0]\n",
    "                \n",
    "                # Calculate prediction differences for each sample\n",
    "                individual_pred_diffs = preds_after_individual.mean(axis=1) - preds_before_individual.mean(axis=1)\n",
    "                ci_lower_diff = np.percentile(individual_pred_diffs, 2.5)\n",
    "                ci_upper_diff = np.percentile(individual_pred_diffs, 97.5)\n",
    "                \n",
    "                print(f\"Individual {i_idx}:\")\n",
    "                print(f\"  Steps change: {original_steps_values[i_idx]:.1f} ‚Üí {perturbed_steps_values[i_idx]:.1f} ({steps_change:+.1f})\")\n",
    "                print(f\"  Avg glucose prediction change: {pred_change:+.4f}\")\n",
    "                print(f\"  95% CI for prediction change: [{ci_lower_diff:+.4f}, {ci_upper_diff:+.4f}]\")\n",
    "        else:\n",
    "            print(\"No individuals meet the step change > 20 threshold.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cDIME",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
